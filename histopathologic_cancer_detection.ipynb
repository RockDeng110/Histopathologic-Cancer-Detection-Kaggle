{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1c2f6186",
      "metadata": {
        "id": "1c2f6186"
      },
      "source": [
        "# **Histopathologic Cancer Detection (Kaggle)**\n",
        "\n",
        "### **Project Objective**\n",
        "\n",
        "To build a deep learning model that accurately identifies metastatic cancer tissue in 96x96px histopathologic image patches. The model must predict the probability that the center 32x32px region contains tumor tissue.\n",
        "\n",
        "**Competition Metric:** ROC AUC\n",
        "\n",
        "### **Project Workflow**\n",
        "\n",
        "This notebook is designed to be flexible and supports two distinct run modes, managed by the CFG class:\n",
        "\n",
        "1. **Training (on Google Colab Pro):**  \n",
        "   * Set CFG.TRAINING \\= True and CFG.INFERENCE \\= False.  \n",
        "   * Run on Colab Pro with high-performance GPUs (V100/A100) and High-RAM.  \n",
        "   * Data will be downloaded via the Kaggle API to the local Colab VM.  \n",
        "   * This mode will run the full K-Fold training and save the resulting model weights.  \n",
        "   * Model weights (.h5 or .pth) should be manually saved (e.g., to Google Drive) and then uploaded as a private **Kaggle Dataset**.  \n",
        "2. **Inference (on Kaggle Notebooks):**  \n",
        "   * Set CFG.TRAINING \\= False and CFG.INFERENCE \\= True.  \n",
        "   * This notebook will be run on the Kaggle platform for submission.  \n",
        "   * It will **not** run training.  \n",
        "   * It will load the pre-trained model weights from the private Kaggle Dataset created in Step 1\\.  \n",
        "   * It will run inference (with TTA) on the official test/ dataset and generate submission.csv."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup & Configuration\n",
        "Import libraries and set system configuration"
      ],
      "metadata": {
        "id": "8FKQ7-EMZIkv"
      },
      "id": "8FKQ7-EMZIkv"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d94b4fb9",
      "metadata": {
        "id": "d94b4fb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "726371b2-57f3-4a87-869d-179465d4da77"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "JAX requires ml_dtypes version 0.5 or newer; installed version is 0.3.2.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3390631835.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m  \u001b[0;31m# 导入 Keras 3.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m \u001b[0;31m# line: 125\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m \u001b[0;31m# line: 125\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/compat/v1/lite/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpsSet\u001b[0m \u001b[0;31m# line: 170\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtoco_convert\u001b[0m \u001b[0;31m# line: 1050\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/compat/v1/lite/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauthoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelAnalyzer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mAnalyzer\u001b[0m \u001b[0;31m# line: 35\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpreter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpResolverType\u001b[0m \u001b[0;31m# line: 303\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/_api/v2/compat/v1/lite/experimental/authoring/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthoring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthoring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompatible\u001b[0m \u001b[0;31m# line: 265\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_wrapper\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/authoring/authoring.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconverter_error_data_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstablehlo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquantization_options_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mquant_opts_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite_constants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwrap_toco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_phase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mComponent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxla_computation\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_xla_computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[0m_xla_computation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Force early import, allowing use of `jax.core` after importing `jax`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# See PEP 484 & https://github.com/jax-ml/jax/issues/7570\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m from jax._src.core import (\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0mAbstractToken\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mAbstractToken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mAbstractValue\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mAbstractValue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meffects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/dtypes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ml_dtypes_version\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     raise ValueError(\"JAX requires ml_dtypes version 0.5 or newer; \"\n\u001b[0m\u001b[1;32m     50\u001b[0m                      f\"installed version is {ml_dtypes.__version__}.\")\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: JAX requires ml_dtypes version 0.5 or newer; installed version is 0.3.2."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras  # 导入 Keras 3.0\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- 1. 配置 ---\n",
        "\n",
        "# 设置 Keras 3.0 后端为 TensorFlow\n",
        "# 你需要在导入 keras 之前设置这个环境变量\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "# 全局常量，用于快速迭代\n",
        "# 竞赛的图像尺寸是 96x96\n",
        "IMAGE_SIZE = 96\n",
        "# 鉴于你的 GPU RAM 很大 (80GB)，你可以使用一个非常大的批量\n",
        "BATCH_SIZE = 512\n",
        "# 快速迭代时，先设置一个较小的 epoch 数\n",
        "EPOCHS = 10\n",
        "SEED = 42\n",
        "\n",
        "# 设置随机种子以保证可复现性\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "keras.utils.set_random_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"--- Keras 3.0 ---\")\n",
        "print(f\"Using Keras backend: {keras.backend.backend()}\")\n",
        "print(f\"Keras version: {keras.__version__}\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n"
      ],
      "metadata": {
        "id": "Iw7v9MXOTS_V"
      },
      "id": "Iw7v9MXOTS_V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Setup\n",
        "* **Download data from my google drive**"
      ],
      "metadata": {
        "id": "d4pSMeL6ZlbP"
      },
      "id": "d4pSMeL6ZlbP"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# 1. 挂载 Google Drive\n",
        "print(\"正在挂载 Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- 2. 定义路径 ---\n",
        "# 你在 Drive 上的 ZIP 文件路径 (根据你的截图)\n",
        "GDRIVE_ZIP_PATH = \"/content/drive/MyDrive/Kaggle_Datasets/histopathologic-cancer/histopathologic-cancer-detection.zip\"\n",
        "\n",
        "# 我们要解压到 Colab 本地的临时路径\n",
        "LOCAL_DATA_PATH = \"/content/data\"\n",
        "\n",
        "# 我们用标签文件作为“是否已解压”的检查点\n",
        "CHECK_FILE_PATH = os.path.join(LOCAL_DATA_PATH, \"train_labels.csv\")\n",
        "\n",
        "# --- 3. 检查是否已解压，如果未解压，则执行解压 ---\n",
        "# 这样你重新运行单元格时，如果文件已在，就不用浪费时间再次解压\n",
        "if not os.path.exists(CHECK_FILE_PATH):\n",
        "    print(f\"在 {LOCAL_DATA_PATH} 未找到数据...\")\n",
        "    print(f\"正在从 Google Drive ({GDRIVE_ZIP_PATH}) 解压文件...\")\n",
        "\n",
        "    # 确保目标目录存在\n",
        "    os.makedirs(LOCAL_DATA_PATH, exist_ok=True)\n",
        "\n",
        "    # 从 Drive 解压到 Colab 本地 (-q 参数是“安静模式”，减少输出)\n",
        "    !unzip -q \"$GDRIVE_ZIP_PATH\" -d \"$LOCAL_DATA_PATH\"\n",
        "\n",
        "    print(\"解压完成！\")\n",
        "else:\n",
        "    print(f\"数据已存在于 {LOCAL_DATA_PATH}。跳过解压。\")\n",
        "\n",
        "# --- 4. 设置你代码中使用的路径 ---\n",
        "# BASE_PATH 现在指向 Colab 的本地路径\n",
        "BASE_PATH = LOCAL_DATA_PATH\n",
        "\n",
        "# 模型权重路径\n",
        "MODEL_WEIGHTS_PATH = \"/content/weights\"\n",
        "if not os.path.exists(MODEL_WEIGHTS_PATH):\n",
        "    os.makedirs(MODEL_WEIGHTS_PATH)\n",
        "\n",
        "TRAIN_DATA_DIR = os.path.join(BASE_PATH, \"train\")\n",
        "TEST_DATA_DIR = os.path.join(BASE_PATH, \"test\")\n",
        "LABELS_CSV_PATH = os.path.join(BASE_PATH, \"train_labels.csv\")\n",
        "\n",
        "\n",
        "if not os.path.exists(TRAIN_DATA_DIR) or not os.path.exists(LABELS_CSV_PATH):\n",
        "    print(f\"Error: Data paths not found.\")\n",
        "    print(f\"Please check: \\nTRAIN_DATA_DIR = {TRAIN_DATA_DIR}\")\n",
        "    print(f\"LABELS_CSV_PATH = {LABELS_CSV_PATH}\")\n",
        "\n",
        "# 1. 加载和拆分数据\n",
        "print(\"Loading and splitting data...\")\n",
        "all_labels_df = pd.read_csv(LABELS_CSV_PATH)\n",
        "\n",
        "# 创建 'filename' 列，包含完整的图像路径\n",
        "all_labels_df['filename'] = all_labels_df['id'].apply(\n",
        "    lambda x: os.path.join(TRAIN_DATA_DIR, x + '.tif')\n",
        ")\n",
        "\n",
        "# 将标签转换为整数\n",
        "all_labels_df['label'] = all_labels_df['label'].astype(int)\n",
        "\n",
        "\n",
        "print(f\"Total training samples: {len(all_labels_df)}\")\n",
        "all_labels_df.head()"
      ],
      "metadata": {
        "id": "2NS_lVc1Zrga"
      },
      "id": "2NS_lVc1Zrga",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2yuJoOyQUg6W"
      },
      "id": "2yuJoOyQUg6W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9sStl6cXUgZE"
      },
      "id": "9sStl6cXUgZE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3\\. EDA**"
      ],
      "metadata": {
        "id": "1syH5NhbZp67"
      },
      "id": "1syH5NhbZp67"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check if the dataset is balanced.**"
      ],
      "metadata": {
        "id": "rZ0C4s65ZdX7"
      },
      "id": "rZ0C4s65ZdX7"
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.countplot(x='label', data=all_labels_df)\n",
        "plt.title('Class Distribution')\n",
        "plt.show()\n",
        "print(all_labels_df['label'].value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "8AoglTML9cCI"
      },
      "id": "8AoglTML9cCI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**View Sample Images**\n",
        "\n",
        "Visualize some positive and negative samples."
      ],
      "metadata": {
        "id": "HzCI-EJx9gqp"
      },
      "id": "HzCI-EJx9gqp"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_samples(df, n=5):\n",
        "    \"\"\"Plots n positive and n negative samples.\"\"\"\n",
        "    pos_samples = df[df['label'] == 1]['filename'].sample(n, random_state=SEED).values\n",
        "    neg_samples = df[df['label'] == 0]['filename'].sample(n, random_state=SEED).values\n",
        "\n",
        "    fig, axes = plt.subplots(2, n, figsize=(n*3, 6))\n",
        "\n",
        "    for i in range(n):\n",
        "        # Plot positive samples\n",
        "        img_pos = plt.imread(pos_samples[i])\n",
        "        axes[0, i].imshow(img_pos)\n",
        "        axes[0, i].set_title(\"Label: 1\")\n",
        "        axes[0, i].axis('off')\n",
        "\n",
        "        # Plot negative samples\n",
        "        img_neg = plt.imread(neg_samples[i])\n",
        "        axes[1, i].imshow(img_neg)\n",
        "        axes[1, i].set_title(\"Label: 0\")\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot samples (only if not in debug mode, or if debug df is large enough)\n",
        "if len(all_labels_df) > 10:\n",
        "    plot_samples(all_labels_df)\n",
        "else:\n",
        "    print(\"Skipping sample plotting due to small debug dataframe.\")"
      ],
      "metadata": {
        "id": "cp8dafqS9hG_"
      },
      "id": "cp8dafqS9hG_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Cleaning**\n",
        "\n",
        "This dataset is known to contain anomalies (e.g., blank slides, slides with markers, duplicates). A simple cleaning step is to remove images with very low variance, as they are often blank."
      ],
      "metadata": {
        "id": "_GpjS40cqFkG"
      },
      "id": "_GpjS40cqFkG"
    },
    {
      "cell_type": "code",
      "source": [
        "# [CODE CELL]\n",
        "import cv2 # <-- 重新导入以确保此单元格能独立运行\n",
        "from joblib import Parallel, delayed # <-- 导入 Joblib\n",
        "import os # <-- 导入 os 以获取 CPU 核心数\n",
        "import matplotlib.pyplot as plt # <-- 导入 plt\n",
        "\n",
        "# This step can be slow. For a quick run (DEBUG=True), we might skip it or run on the sample.\n",
        "# For the full dataset, this should be run once.\n",
        "\n",
        "def get_image_variance(image_path):\n",
        "    \"\"\"Calculates the variance of a single image.\"\"\"\n",
        "    try:\n",
        "        # Read image in grayscale which is enough for variance calculation\n",
        "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if image is None:\n",
        "            return 0\n",
        "        return image.var()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {image_path}: {e}\")\n",
        "        return 0\n",
        "\n",
        "print(\"Starting data cleaning (calculating variance)... This may take a while.\")\n",
        "\n",
        "# --- 优化：并行处理 ---\n",
        "# 利用 Colab Pro 的多核心 CPU 和高 RAM\n",
        "# 替代慢速的: df_labels['file_path'].apply(get_image_variance)\n",
        "\n",
        "n_cores = os.cpu_count()\n",
        "print(f\"Utilizing {n_cores} CPU cores for parallel variance calculation.\")\n",
        "\n",
        "paths_to_process = all_labels_df['filename'].values\n",
        "\n",
        "# 使用 joblib 并行运行, verbose=5 会显示进度\n",
        "variances = Parallel(n_jobs=n_cores, verbose=5)(\n",
        "    delayed(get_image_variance)(path) for path in paths_to_process\n",
        ")\n",
        "\n",
        "# 将结果分配回 dataframe\n",
        "all_labels_df['variance'] = variances\n",
        "\n",
        "print(\"Variance calculation complete.\")\n",
        "\n",
        "# --- 1. (新增) 可视化方差最低的10个样本 ---\n",
        "print(\"Displaying 10 samples with the lowest variance...\")\n",
        "low_variance_samples = all_labels_df.sort_values(by='variance').head(10)\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "axes = axes.flatten()\n",
        "for i, (idx, row) in enumerate(low_variance_samples.iterrows()):\n",
        "    try:\n",
        "        img = plt.imread(row['filename'])\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].set_title(f\"Var: {row['variance']:.2f}\\nLabel: {row['label']}\")\n",
        "        axes[i].axis('off')\n",
        "    except Exception as e:\n",
        "        axes[i].set_title(f\"Error loading img: {row['id']}\")\n",
        "        print(e)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot variance distribution\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.histplot(all_labels_df['variance'], bins=100, kde=True)\n",
        "plt.title('Image Variance Distribution')\n",
        "plt.show()\n",
        "\n",
        "# --- 2. (修改) 过滤低方差图像并使用新变量 ---\n",
        "VARIANCE_THRESHOLD = 100 # This is a common threshold, can be tuned\n",
        "original_count = len(all_labels_df)\n",
        "\n",
        "# 将清理后的数据存储在新 DataFrame 'df_labels_clean' 中\n",
        "all_labels_df_clean = all_labels_df[all_labels_df['variance'] > VARIANCE_THRESHOLD].reset_index(drop=True)\n",
        "new_count = len(all_labels_df_clean)\n",
        "\n",
        "print(f\"--- Data Cleaning Report ---\")\n",
        "print(f\"Original sample count (all_labels_df): {original_count}\")\n",
        "print(f\"Removed {original_count - new_count} low-variance images (var < {VARIANCE_THRESHOLD}).\")\n",
        "print(f\"New sample count (all_labels_df_clean): {new_count}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "vW1kfpPEqG3E"
      },
      "id": "vW1kfpPEqG3E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# --- 2. 数据加载与预处理 (使用 tf.data) ---\n",
        "\n",
        "def parse_image(filename, label):\n",
        "    \"\"\"\n",
        "    加载并预处理单个图像。\n",
        "    \"\"\"\n",
        "    try:\n",
        "        image = tf.io.read_file(filename)\n",
        "        # 关键：使用 decode_tiff 来读取 .tif 文件\n",
        "        # 这个竞赛的图像是 96x96x3\n",
        "        image = tf.io.decode_tiff(image)\n",
        "        # 确保图像是 3 通道 (RGB)\n",
        "        if image.shape.ndims == 4:\n",
        "            # Tiff 可能是多页的，取第一页\n",
        "            image = image[0]\n",
        "        if image.shape[-1] == 4:\n",
        "            # 去掉 Alpha 通道\n",
        "            image = image[:, :, :3]\n",
        "\n",
        "        # 确保输出形状一致\n",
        "        image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n",
        "        # 转换数据类型为 float32 (0-1)\n",
        "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing image {filename}: {e}\")\n",
        "        # 返回一个空图像或占位符\n",
        "        image = tf.zeros([IMAGE_SIZE, IMAGE_SIZE, 3], dtype=tf.float32)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "def create_dataset(df, is_training=True):\n",
        "    \"\"\"\n",
        "    根据 DataFrame 创建一个 tf.data.Dataset。\n",
        "    \"\"\"\n",
        "    # 从 DataFrame 的 'filename' 和 'label' 列创建数据集\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((df['filename'].values, df['label'].values))\n",
        "\n",
        "    # 使用多核并行处理图像\n",
        "    dataset = dataset.map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    if is_training:\n",
        "        # 仅在训练时：\n",
        "        # 1. 缓存数据（如果内存允许）\n",
        "        # 2. 打乱数据\n",
        "        dataset = dataset.cache()\n",
        "        dataset = dataset.shuffle(buffer_size=len(df), seed=SEED)\n",
        "\n",
        "    # 批量处理\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "\n",
        "    if not is_training:\n",
        "        # 验证集在批处理后缓存，以加快评估速度\n",
        "        dataset = dataset.cache()\n",
        "\n",
        "    # 预取数据以获得最佳性能\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# --- 3. 模型构建 ---\n",
        "\n",
        "def build_augmentation_model():\n",
        "    \"\"\"\n",
        "    创建包含 Keras 预处理层的数据增强模型。\n",
        "    这将利用 GPU 进行加速。\n",
        "    \"\"\"\n",
        "    return keras.Sequential([\n",
        "        keras.layers.RandomFlip(\"horizontal_and_vertical\", seed=SEED),\n",
        "        keras.layers.RandomRotation(0.2, seed=SEED),\n",
        "        keras.layers.RandomZoom(0.2, seed=SEED),\n",
        "        keras.layers.RandomTranslation(0.2, 0.2, seed=SEED),\n",
        "    ], name=\"augmentation_model\")\n",
        "\n",
        "def build_model(base_model_name, augmentation_model):\n",
        "    \"\"\"\n",
        "    构建、编译一个用于快速比较的模型。\n",
        "    主干网络将被冻结。\n",
        "    \"\"\"\n",
        "    inputs = keras.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
        "\n",
        "    # 1. 数据增强层\n",
        "    x = augmentation_model(inputs)\n",
        "\n",
        "    # 2. 选择基础模型\n",
        "    if base_model_name == 'EfficientNetV2B0':\n",
        "        base_model = keras.applications.EfficientNetV2B0(\n",
        "            include_top=False, weights='imagenet', input_tensor=x\n",
        "        )\n",
        "    elif base_model_name == 'MobileNetV3Small':\n",
        "        base_model = keras.applications.MobileNetV3Small(\n",
        "            include_top=False, weights='imagenet', input_tensor=x\n",
        "        )\n",
        "    elif base_model_name == 'ResNet50V2':\n",
        "        base_model = keras.applications.ResNet50V2(\n",
        "            include_top=False, weights='imagenet', input_tensor=x\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown base model: {base_model_name}\")\n",
        "\n",
        "    # 3. 冻结主干网络\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # 4. 添加分类头\n",
        "    x = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    x = keras.layers.Dropout(0.3)(x)\n",
        "    # 二分类，使用 sigmoid 激活函数\n",
        "    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # 5. 编译模型\n",
        "    # 学习率先用一个标准值\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "    # 竞赛的核心指标是 AUC\n",
        "    metrics = [\n",
        "        keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "        keras.metrics.AUC(name='auc')\n",
        "    ]\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=metrics\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6aL0iNSVOVMi"
      },
      "id": "6aL0iNSVOVMi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. 主执行函数 ---\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    执行模型选择的主流程。\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # 过滤掉不存在的文件（如果需要）\n",
        "    # (在你的EDA中可能已经做过)\n",
        "\n",
        "    # 拆分训练集和验证集 (80/20)\n",
        "    # 使用 stratify 来保持标签分布\n",
        "    train_df, val_df = train_test_split(\n",
        "        all_labels_df_clean,\n",
        "        test_size=0.2,\n",
        "        random_state=SEED,\n",
        "        stratify=all_labels_df_clean['label']\n",
        "    )\n",
        "\n",
        "    print(f\"Total samples: {len(all_labels_df_clean)}\")\n",
        "    print(f\"Training samples: {len(train_df)}\")\n",
        "    print(f\"Validation samples: {len(val_df)}\")\n",
        "\n",
        "    # 2. 创建 tf.data pipelines\n",
        "    print(\"Creating data pipelines...\")\n",
        "    train_dataset = create_dataset(train_df, is_training=True)\n",
        "    val_dataset = create_dataset(val_df, is_training=False)\n",
        "    print(\"Data pipelines created.\")\n",
        "\n",
        "    # 3. 定义要比较的模型\n",
        "    # 选择一些轻量级且性能优秀的基础模型\n",
        "    models_to_try = ['MobileNetV3Small', 'EfficientNetV2B0', 'ResNet50V2']\n",
        "    histories = {}\n",
        "\n",
        "    # 4. 构建数据增强\n",
        "    augmentation_model = build_augmentation_model()\n",
        "\n",
        "    # 5. 循环训练和评估模型\n",
        "    print(\"Starting model training loop...\")\n",
        "\n",
        "    # 定义回调\n",
        "    callbacks = [\n",
        "        # 监控 val_auc，5 个 epoch 没有提升就停止\n",
        "        keras.callbacks.EarlyStopping(\n",
        "            monitor='val_auc',\n",
        "            patience=5,\n",
        "            mode='max',\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        # 当 val_auc 停滞时，自动降低学习率\n",
        "        keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_auc',\n",
        "            factor=0.2,\n",
        "            patience=2,\n",
        "            mode='max'\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    for model_name in models_to_try:\n",
        "        print(f\"\\n--- Training {model_name} ---\")\n",
        "        # 清理 Keras 会话，释放内存\n",
        "        keras.utils.clear_session()\n",
        "\n",
        "        model = build_model(model_name, augmentation_model)\n",
        "\n",
        "        # if model_name == models_to_try[0]:\n",
        "        # model.summary()  # 只打印第一个模型的摘要\n",
        "\n",
        "        history = model.fit(\n",
        "            train_dataset,\n",
        "            epochs=EPOCHS,\n",
        "            validation_data=val_dataset,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1  # 打印进度\n",
        "        )\n",
        "\n",
        "        histories[model_name] = history\n",
        "        best_val_auc = max(history.history['val_auc'])\n",
        "        print(f\"--- Best Validation AUC for {model_name}: {best_val_auc:.5f} ---\")\n",
        "\n",
        "    # 6. 总结结果\n",
        "    print(\"\\n--- Model Comparison Summary (Best Val AUC) ---\")\n",
        "    summary_results = {}\n",
        "    for model_name, history in histories.items():\n",
        "        best_auc = max(history.history['val_auc'])\n",
        "        summary_results[model_name] = best_auc\n",
        "        print(f\"{model_name}: {best_auc:.5f}\")\n",
        "\n",
        "    best_model_name = max(summary_results, key=summary_results.get)\n",
        "    print(f\"\\n--- Best Performing Model: {best_model_name} ---\")\n",
        "\n",
        "    print(\"\\nStep 1 (Model Selection) Complete.\")\n",
        "    print(f\"Next step: Take '{best_model_name}' and move to Step 2 (Fine-Tuning & Hyperparameter Tuning).\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "eW7KKOb1g69y"
      },
      "id": "eW7KKOb1g69y",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}